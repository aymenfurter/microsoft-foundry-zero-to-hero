{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a05caafc",
   "metadata": {},
   "source": [
    "# Lab 2A: Deploy Team Spokes with Model Access\n",
    "\n",
    "Deploy **Team Spokes** that connect to the centralized Landing Zone via APIM gateway.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f605c01",
   "metadata": {},
   "source": [
    "## Step 0: Deploy Models to Landing Zone\n",
    "\n",
    "Ensure required models are deployed with **correct format per provider**:\n",
    "\n",
    "| Provider | Models | Format |\n",
    "|----------|--------|--------|\n",
    "| OpenAI | gpt-4.1, gpt-4o, model-router | `OpenAI` |\n",
    "| xAI | grok-3 | `xAI` |\n",
    "| DeepSeek | DeepSeek-R1 | `DeepSeek` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46125259",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import get_lz_account, get_existing_deployments, deploy_model, REQUIRED_MODELS\n",
    "\n",
    "LZ_RG = \"foundry-lz-parent\"\n",
    "LZ_ACCOUNT = get_lz_account(LZ_RG)\n",
    "existing = get_existing_deployments(LZ_RG, LZ_ACCOUNT)\n",
    "\n",
    "print(f\"Landing Zone: {LZ_ACCOUNT}\")\n",
    "print(f\"Existing: {existing}\\n\")\n",
    "\n",
    "for model in REQUIRED_MODELS:\n",
    "    if model['name'] in existing:\n",
    "        print(f\"‚úÖ {model['name']}\")\n",
    "    else:\n",
    "        print(f\"üöÄ {model['name']} ({model['format']})...\", end=\" \", flush=True)\n",
    "        ok, err = deploy_model(LZ_RG, LZ_ACCOUNT, model)\n",
    "        print(\"‚úÖ\" if ok else f\"‚ùå {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa611506",
   "metadata": {},
   "source": [
    "## Step 1: Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58838c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import load_env, load_spoke_config, get_principal_id\n",
    "\n",
    "APIM_URL, APIM_KEY = load_env()\n",
    "config = load_spoke_config()\n",
    "PRINCIPAL_ID = get_principal_id()\n",
    "\n",
    "print(f\"APIM: {APIM_URL}\")\n",
    "print(f\"Teams: {[s['displayName'] for s in config['spokes']]}\")\n",
    "print(f\"Principal: {PRINCIPAL_ID[:8]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a0dd0e",
   "metadata": {},
   "source": [
    "## Step 2: Deploy Team Spokes\n",
    "\n",
    "Each spoke gets its own AI Foundry Account + APIM connection to the Landing Zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75cae2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import deploy_spoke, save_deployments\n",
    "\n",
    "deployed_teams = []\n",
    "\n",
    "for spoke in config['spokes']:\n",
    "    print(f\"üöÄ {spoke['displayName']}...\", end=\" \", flush=True)\n",
    "    result = deploy_spoke(spoke, PRINCIPAL_ID, APIM_URL, APIM_KEY)\n",
    "    if result:\n",
    "        deployed_teams.append(result)\n",
    "        print(f\"‚úÖ {result['accountName']}\")\n",
    "    else:\n",
    "        print(\"‚ùå\")\n",
    "\n",
    "save_deployments(deployed_teams)\n",
    "print(f\"\\n‚úÖ Deployed {len(deployed_teams)}/{len(config['spokes'])} teams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c418811",
   "metadata": {},
   "source": [
    "## Step 3: Test Model Access via Agent API\n",
    "\n",
    "**Key Pattern**: Spoke projects access models via `<connection>/<model>` format using the **Agent/Responses API**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0066633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install azure-ai-projects==2.0.0b2 azure-identity -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "729ace10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üè¢ Contoso Ltd\n",
      "  üìÅ inventory-ai: gpt-4.1-mini, gpt-4.1, model-router\n",
      "     ‚úÖ gpt-4.1-mini: The largest planet in our solar system i...\n",
      "     ‚úÖ gpt-4.1: The largest planet in our solar system i...\n",
      "     ‚úÖ model-router: The largest planet in our solar system i...\n",
      "\n",
      "üè¢ Fabrikam Inc\n",
      "  üìÅ doc-studio: gpt-4o, gpt-4o-mini, grok-3\n",
      "     ‚úÖ gpt-4o: The largest planet in our solar system i...\n",
      "     ‚úÖ gpt-4o-mini: The largest planet in our solar system i...\n",
      "     ‚úÖ grok-3: Jupiter is the largest planet in our sol...\n",
      "\n",
      "üè¢ Woodgrove Bank\n",
      "  üìÅ risk-analysis: grok-3, DeepSeek-R1, gpt-4.1-mini\n",
      "     ‚úÖ grok-3: Jupiter is the largest planet in our sol...\n",
      "     ‚úÖ DeepSeek-R1: **Jupiter** is the largest planet in our...\n",
      "     ‚úÖ gpt-4.1-mini: The largest planet in our solar system i...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.projects.models import PromptAgentDefinition\n",
    "from helpers import make_agent_name\n",
    "\n",
    "def extract_response(text):\n",
    "    \"\"\"Strip DeepSeek <think> tags and leading whitespace\"\"\"\n",
    "    text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)\n",
    "    return text.strip()\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "for team in deployed_teams:\n",
    "    print(f\"\\nüè¢ {team['displayName']}\")\n",
    "    spoke_cfg = next(s for s in config['spokes'] if s['name'] == team['name'])\n",
    "    \n",
    "    for i, endpoint in enumerate(team['projectEndpoints']):\n",
    "        proj = spoke_cfg['projects'][i]\n",
    "        print(f\"  üìÅ {proj['name']}: {', '.join(proj['allowedModels'])}\")\n",
    "        \n",
    "        client = AIProjectClient(credential=credential, endpoint=endpoint)\n",
    "        openai = client.get_openai_client()\n",
    "        \n",
    "        for model in proj['allowedModels']:\n",
    "            gateway_model = f\"{team['connectionName']}/{model}\"\n",
    "            agent_name = make_agent_name(team['name'], proj['name'], model)\n",
    "            \n",
    "            try:\n",
    "                agent = client.agents.create_version(\n",
    "                    agent_name=agent_name,\n",
    "                    definition=PromptAgentDefinition(model=gateway_model, instructions=\"You are a space exploration expert. Answer briefly.\")\n",
    "                )\n",
    "                resp = openai.responses.create(\n",
    "                    input=\"What is the largest planet in our solar system?\",\n",
    "                    extra_body={\"agent\": {\"name\": agent.name, \"version\": agent.version, \"type\": \"agent_reference\"}}\n",
    "                )\n",
    "                text = extract_response(resp.output_text)\n",
    "                print(f\"     ‚úÖ {model}: {text[:40]}...\")\n",
    "            except Exception as e:\n",
    "                print(f\"     ‚ùå {model}: {str(e)[:60]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740abb9f",
   "metadata": {},
   "source": [
    "## Why Agent API? Direct Chat Completions Fail\n",
    "\n",
    "APIM connections **require** the Agent/Responses API. Direct `chat.completions` won't work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa079d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "team = deployed_teams[0]\n",
    "spoke_cfg = next(s for s in config['spokes'] if s['name'] == team['name'])\n",
    "model = spoke_cfg['projects'][0]['allowedModels'][0]\n",
    "gateway_model = f\"{team['connectionName']}/{model}\"\n",
    "\n",
    "client = AIProjectClient(credential=credential, endpoint=team['projectEndpoints'][0])\n",
    "openai = client.get_openai_client()\n",
    "\n",
    "print(f\"Testing: {gateway_model}\\n\")\n",
    "\n",
    "# ‚ùå Direct chat completions - FAILS\n",
    "print(\"‚ùå chat.completions.create():\")\n",
    "try:\n",
    "    resp = openai.chat.completions.create(model=gateway_model, messages=[{\"role\": \"user\", \"content\": \"Name a planet with rings.\"}])\n",
    "    print(f\"   {resp.choices[0].message.content[:40]}\")\n",
    "except Exception as e:\n",
    "    print(f\"   Error: {str(e)[:80]}...\")\n",
    "\n",
    "# ‚úÖ Agent + Responses API - WORKS\n",
    "print(\"\\n‚úÖ Agent + responses.create():\")\n",
    "agent = client.agents.create_version(\n",
    "    agent_name=\"demoagent\",\n",
    "    definition=PromptAgentDefinition(model=gateway_model, instructions=\"You are a space exploration expert. Answer briefly.\")\n",
    ")\n",
    "resp = openai.responses.create(\n",
    "    input=\"Name a planet with rings.\",\n",
    "    extra_body={\"agent\": {\"name\": agent.name, \"version\": agent.version, \"type\": \"agent_reference\"}}\n",
    ")\n",
    "print(f\"   {resp.output_text[:40]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59871652",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| What | How |\n",
    "|------|-----|\n",
    "| Model format | Use provider-specific: `OpenAI`, `xAI`, `DeepSeek` |\n",
    "| Gateway model | `<connection-name>/<model-name>` |\n",
    "| API pattern | Must use Agent + `responses.create()` |\n",
    "| Agent naming | Alphanumeric only, no hyphens at edges |\n",
    "\n",
    "**Next**: Lab 2B for direct APIM access without Foundry spoke"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
