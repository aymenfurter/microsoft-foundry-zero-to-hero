{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a47cff82",
   "metadata": {},
   "source": [
    "# Lab 1C: Add Model Router to Landing Zone\n",
    "\n",
    "Deploy the **Model Router** to the Landing Zone and enable it for project spokes.\n",
    "\n",
    "## What Gets Deployed\n",
    "\n",
    "| Resource | Purpose |\n",
    "|----------|--------|\n",
    "| Model Router Deployment | Intelligent model selection that routes queries to the optimal model |\n",
    "\n",
    "## What is Model Router?\n",
    "\n",
    "Model Router automatically selects the best model for each query based on:\n",
    "- **Performance**: Matches query complexity to model capabilities\n",
    "- **Cost**: Uses smaller models for simple queries, larger models for complex ones  \n",
    "- **Latency**: Optimizes response time by routing appropriately\n",
    "\n",
    "> ‚ö†Ô∏è **Prerequisite**: Complete **Lab 1A** and **Lab 1B** first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e230a68d",
   "metadata": {},
   "source": [
    "## Step 1: Load Existing Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572a0f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "env_file = '/workspaces/getting-started-with-foundry/.env'\n",
    "with open(env_file) as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line and not line.startswith('#') and '=' in line:\n",
    "            key, value = line.split('=', 1)\n",
    "            os.environ[key] = value\n",
    "\n",
    "AI_ENDPOINT = os.environ['AI_ENDPOINT']\n",
    "APIM_URL = os.environ['APIM_URL']\n",
    "APIM_KEY = os.environ['APIM_KEY']\n",
    "MODEL_NAME = os.environ['MODEL_NAME']\n",
    "\n",
    "print(f\"AI Endpoint: {AI_ENDPOINT}\\nAPIM URL: {APIM_URL}\\nModel: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd67b375",
   "metadata": {},
   "source": [
    "## Step 2: Set Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdeb7101",
   "metadata": {},
   "outputs": [],
   "source": [
    "RG = \"foundry-lz-parent\"\n",
    "LOCATION = \"eastus2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3d0306",
   "metadata": {},
   "source": [
    "## Step 3: Deploy Model Router and gpt-4.1-nano to Landing Zone\n",
    "\n",
    "Model Router requires underlying models to route to. We'll add gpt-4.1-nano for simple queries.\n",
    "\n",
    "‚è±Ô∏è Takes ~1-2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3507753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "!az deployment group create -g \"{RG}\" --template-file main.bicep -o table\n",
    "\n",
    "AI_ACCOUNT_NAME = subprocess.run(f'az deployment group show -g \"{RG}\" -n main --query properties.outputs.aiAccountName.value -o tsv', \n",
    "                                  shell=True, capture_output=True, text=True).stdout.strip()\n",
    "\n",
    "print(f\"\\nDeploying gpt-4.1-nano to {AI_ACCOUNT_NAME}...\")\n",
    "!az cognitiveservices account deployment create -g \"{RG}\" -n \"{AI_ACCOUNT_NAME}\" \\\n",
    "  --deployment-name gpt-4.1-nano --model-name gpt-4.1-nano --model-version 2025-04-14 \\\n",
    "  --model-format OpenAI --sku-capacity 30 --sku-name GlobalStandard -o table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529fe020",
   "metadata": {},
   "source": [
    "## Step 4: Get Model Router Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa07e0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model Router: model-router\n"
     ]
    }
   ],
   "source": [
    "import subprocess, json\n",
    "from pathlib import Path\n",
    "\n",
    "r = subprocess.run(f'az deployment group show -g \"{RG}\" -n main --query properties.outputs -o json', shell=True, capture_output=True, text=True)\n",
    "MODEL_ROUTER_NAME = json.loads(r.stdout)['modelRouterName']['value']\n",
    "\n",
    "env_file = Path(\"/workspaces/getting-started-with-foundry/.env\")\n",
    "with open(env_file, 'a') as f:\n",
    "    f.write(f\"\\nMODEL_ROUTER_NAME={MODEL_ROUTER_NAME}\\n\")\n",
    "\n",
    "print(f\"‚úÖ Model Router: {MODEL_ROUTER_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e8db46",
   "metadata": {},
   "source": [
    "## Step 5: Test Model Router Directly (Landing Zone)\n",
    "\n",
    "First, test that the model router is working directly against the landing zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8d7733",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai azure-identity -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7423069d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple query ‚Üí Model: gpt-4.1-nano-2025-04-14\n",
      "Response: 2 + 2 equals 4.\n"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n",
    "client = AzureOpenAI(azure_endpoint=AI_ENDPOINT, azure_ad_token_provider=token_provider, api_version=\"2024-10-21\")\n",
    "\n",
    "response = client.chat.completions.create(model=MODEL_ROUTER_NAME, messages=[{\"role\": \"user\", \"content\": \"What is 2+2?\"}])\n",
    "print(f\"Simple query ‚Üí Model: {response.model}\\nResponse: {response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb232e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complex query ‚Üí Model: gpt-4.1-mini-2025-04-14\n",
      "Response: Certainly! Here's an explanation of supervised vs. unsupervised machine learning along with common use cases:\n",
      "\n",
      "### Supervised Learning\n",
      "\n",
      "**Definition:**  \n",
      "Supervised learning is a type of machine learning where the algorithm is trained on a labeled dataset. This means that each training example is paired with an output label. The goal is for the model to learn a mapping from inputs to outputs so it...\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(model=MODEL_ROUTER_NAME, \n",
    "    messages=[{\"role\": \"user\", \"content\": \"Explain supervised vs unsupervised machine learning with use cases.\"}])\n",
    "print(f\"Complex query ‚Üí Model: {response.model}\\nResponse: {response.choices[0].message.content[:400]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabcf07e",
   "metadata": {},
   "source": [
    "## Step 6: Update Spoke Project Connection\n",
    "\n",
    "Now we need to update the spoke project's APIM connection to include the model-router in its available models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb63eabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPOKE_ACCOUNT = os.environ.get('SPOKE_ACCOUNT')\n",
    "SPOKE_PROJECT = os.environ.get('SPOKE_PROJECT')\n",
    "APIM_CONNECTION = os.environ.get('APIM_CONNECTION')\n",
    "\n",
    "if SPOKE_ACCOUNT:\n",
    "    print(f\"Spoke: {SPOKE_ACCOUNT} / {SPOKE_PROJECT}\\nConnection: {APIM_CONNECTION}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Run Lab 1B first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3f07da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connection updated with models: gpt-4.1-mini, model-router\n"
     ]
    }
   ],
   "source": [
    "import subprocess, json, tempfile\n",
    "\n",
    "if SPOKE_ACCOUNT:\n",
    "    SPOKE_RG = \"foundry-child-1\"\n",
    "    models_config = [\n",
    "        {\"name\": MODEL_NAME, \"properties\": {\"model\": {\"name\": MODEL_NAME, \"version\": \"\", \"format\": \"OpenAI\"}}},\n",
    "        {\"name\": MODEL_ROUTER_NAME, \"properties\": {\"model\": {\"name\": MODEL_ROUTER_NAME, \"version\": \"\", \"format\": \"OpenAI\"}}}\n",
    "    ]\n",
    "    \n",
    "    sub_id = subprocess.run('az account show --query id -o tsv', shell=True, capture_output=True, text=True).stdout.strip()\n",
    "    connection_uri = f\"https://management.azure.com/subscriptions/{sub_id}/resourceGroups/{SPOKE_RG}/providers/Microsoft.CognitiveServices/accounts/{SPOKE_ACCOUNT}/projects/{SPOKE_PROJECT}/connections/{APIM_CONNECTION}?api-version=2025-04-01-preview\"\n",
    "    \n",
    "    existing = json.loads(subprocess.run(f'az rest --method GET --uri \"{connection_uri}\" -o json', shell=True, capture_output=True, text=True).stdout)\n",
    "    \n",
    "    update_body = {\"properties\": {\"category\": \"ApiManagement\", \"target\": existing['properties']['target'], \"authType\": \"ApiKey\",\n",
    "        \"credentials\": {\"key\": APIM_KEY}, \"metadata\": {\"deploymentInPath\": \"true\", \"inferenceAPIVersion\": \"2024-10-21\", \"models\": json.dumps(models_config)}}}\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:\n",
    "        json.dump(update_body, f); body_file = f.name\n",
    "    \n",
    "    result = subprocess.run(f'az rest --method PUT --uri \"{connection_uri}\" --body @{body_file}', shell=True, capture_output=True, text=True)\n",
    "    os.unlink(body_file)\n",
    "    \n",
    "    print(f\"‚úÖ Connection updated with models: {MODEL_NAME}, {MODEL_ROUTER_NAME}\" if result.returncode == 0 else f\"‚ùå Error: {result.stderr}\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping - run Lab 1B first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539a745e",
   "metadata": {},
   "source": [
    "## Step 7: Test Model Router from Spoke via Agent\n",
    "\n",
    "Test that the spoke project can access model-router via the APIM gateway using the Agent/Responses API.\n",
    "\n",
    "> **Note**: In spoke projects, you must use agents with the Responses API - direct chat completions don't work with APIM connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "296c2e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install azure-ai-projects==2.0.0b2 azure-identity -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18e673a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Agent: model-router-agent v1 using landing-zone-apim/model-router\n"
     ]
    }
   ],
   "source": [
    "if SPOKE_ACCOUNT:\n",
    "    from azure.identity import DefaultAzureCredential\n",
    "    from azure.ai.projects import AIProjectClient\n",
    "    from azure.ai.projects.models import PromptAgentDefinition\n",
    "    \n",
    "    SPOKE_ENDPOINT = os.environ.get('SPOKE_ENDPOINT', '')\n",
    "    account_host = SPOKE_ENDPOINT.replace(\"https://\", \"\").replace(\".cognitiveservices.azure.com/\", \"\")\n",
    "    PROJECT_ENDPOINT = f\"https://{account_host}.services.ai.azure.com/api/projects/{SPOKE_PROJECT}\"\n",
    "    \n",
    "    project_client = AIProjectClient(credential=DefaultAzureCredential(), endpoint=PROJECT_ENDPOINT)\n",
    "    openai_client = project_client.get_openai_client()\n",
    "    \n",
    "    GATEWAY_MODEL_ROUTER = f\"{APIM_CONNECTION}/{MODEL_ROUTER_NAME}\"\n",
    "    agent = project_client.agents.create_version(agent_name=\"model-router-agent\",\n",
    "        definition=PromptAgentDefinition(model=GATEWAY_MODEL_ROUTER, instructions=\"You are a helpful assistant.\"))\n",
    "    \n",
    "    print(f\"‚úÖ Agent: {agent.name} v{agent.version} using {GATEWAY_MODEL_ROUTER}\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping - run Lab 1B first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7542595e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## üß™ Model Router Verification"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>Query Type</th><th>Expected</th><th>Actual Model</th><th>Result</th></tr><tr><td>Simple</td><td>nano</td><td>gpt-4.1-nano-2025-04-14</td><td>‚úÖ</td></tr><tr><td>Complex</td><td>mini</td><td>gpt-4.1-mini-2025-04-14</td><td>‚úÖ</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### üìã Sample Responses"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Simple:** 2 + 2 = 4..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Complex:** Certainly! Here's an explanation of supervised vs unsupervised machine learning, including common algorithms and use cases for each.\n",
       "\n",
       "---\n",
       "\n",
       "## Supervis..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if SPOKE_ACCOUNT:\n",
    "    from IPython.display import display, Markdown, HTML\n",
    "    \n",
    "    def test_query(query, label, expected_model):\n",
    "        response = openai_client.responses.create(input=query,\n",
    "            extra_body={\"agent\": {\"name\": agent.name, \"version\": agent.version, \"type\": \"agent_reference\"}})\n",
    "        text = response.output_text if hasattr(response, 'output_text') else str(response.output)\n",
    "        model = response.model\n",
    "        match = \"‚úÖ\" if expected_model in model else \"‚ùå\"\n",
    "        return {\"label\": label, \"expected\": expected_model, \"model\": model, \"match\": match, \"response\": text[:150]}\n",
    "    \n",
    "    results = [\n",
    "        test_query(\"What is 2+2?\", \"Simple\", \"nano\"),\n",
    "        test_query(\"Explain supervised vs unsupervised ML with algorithms and use cases.\", \"Complex\", \"mini\")\n",
    "    ]\n",
    "    \n",
    "    display(Markdown(\"## üß™ Model Router Verification\"))\n",
    "    \n",
    "    rows = \"\".join(f\"<tr><td>{r['label']}</td><td>{r['expected']}</td><td>{r['model']}</td><td>{r['match']}</td></tr>\" for r in results)\n",
    "    display(HTML(f\"<table><tr><th>Query Type</th><th>Expected</th><th>Actual Model</th><th>Result</th></tr>{rows}</table>\"))\n",
    "    \n",
    "    display(Markdown(\"### üìã Sample Responses\"))\n",
    "    for r in results:\n",
    "        display(Markdown(f\"**{r['label']}:** {r['response']}...\"))\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a7797b",
   "metadata": {},
   "source": [
    "## Done!\n",
    "\n",
    "Model Router is now deployed in the landing zone and accessible from spoke projects via Agents.\n",
    "\n",
    "### Verified Model Routing Behavior\n",
    "\n",
    "| Query Type | Expected Model | Why |\n",
    "|------------|----------------|-----|\n",
    "| Simple (e.g., \"What is 2+2?\") | `gpt-4.1-nano` | Cost-effective for trivial queries |\n",
    "| Complex (e.g., ML explanation) | `gpt-4.1-mini` | More capable for detailed responses |\n",
    "\n",
    "### Key Patterns\n",
    "\n",
    "| Pattern | Description |\n",
    "|---------|-------------|\n",
    "| Gateway Model Format | `<connection-name>/<model-id>` (e.g., `landing-zone-apim/model-router`) |\n",
    "| Agent Required | Spoke projects must use Agent/Responses API, not direct chat completions |\n",
    "| PromptAgentDefinition | Defines agent with model and instructions |\n",
    "| Responses API | Invoke agents via `openai_client.responses.create()` with `agent_reference` |\n",
    "\n",
    "### Model Router Pool (v2025-05-19)\n",
    "\n",
    "Model Router can route to: GPT-4.1, GPT-4.1-mini, GPT-4.1-nano, o4-mini\n",
    "\n",
    "---\n",
    "\n",
    "## Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fb3ea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove model-router deployment:\n",
    "# !az cognitiveservices account deployment delete -g \"{RG}\" -n \"foundry-hub-{suffix}\" --deployment-name \"model-router\" --yes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
